{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"\"   # üîë Put your key here\n",
    "client = geniai.configure(api_key=os.environ.get(\"Gemini-1.5-flash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8c3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_video(video_path):\n",
    "    print(\"üé¨ Extracting audio from video...\")\n",
    "    audio_path = \"output_audio.mp3\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.audio.write_audiofile(audio_path)\n",
    "\n",
    "    print(\"üìù Transcribing audio with Whisper...\")\n",
    "    with open(audio_path, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=file,\n",
    "            model=\"whisper-large-v3-turbo\",\n",
    "            response_format=\"text\",\n",
    "            language=\"en\"\n",
    "        )\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55e2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(transcript_text):\n",
    "    print(\"üìå Summarizing transcript with LLaMA...\")\n",
    "    summary_response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Summarize this meeting transcript:\\n{transcript_text}\"}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return summary_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bba683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Extracting audio from video...\n",
      "MoviePy - Writing audio in output_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "üìù Transcribing audio with Whisper...\n",
      "\n",
      "--- FULL TRANSCRIPT ---\n",
      "\n",
      " then fourth table then of the table I think last table is date right of the table you want we are done listing then finally since so here first change the im route to all the things Ctrl-C then change the region change your bucket name okay so what do you need to notice here is okay so I think when you you don't feel any syntax wise difference anything any changes here for when you table this one anything is just like a normal one category also just like a normal one okay so date also just like a normal one okay if you go for the next one okay for even table just check for it because of the time format is different over there we need to give the exact time format whatever that was in our file okay so then for comma separated I think tab separated file okay the last one just check for it what is the delimiter here slash okay so these are all the things you need to do okay just copy and executed in your corresponding day I got a confidence about my syntax everything is done so everything loaded data successfully this This is how you need to load the data into your corresponding headship. So this is a part of loading process. So next one example I will show you for unloading. For unloading, let me show you one example. Most probably, just listen, it is just like a normal data warehousing tool, whatever you studied so far just like a normal data warehousing tool because of the cloud what we learned here is how to integrate data lake with your redshift how to give the proper permission how to load the data from so now we are doing etl or elt e e lt so already data is loaded and that will be available in my bucket bucket from bucket to redshift i'm just loading if you want you can do the transformation and you can dump it in your s3 or else you can use just like a normal thing and you can use it okay this is the difference between other tools and redshift our main key thing is we need to concentrate more on integration part integration how to link redshift with s3 how to link redshift with rds how to link redshift with ec2 so these kind of integration part and all you need to very clear in all those things most probably your project redshift project if I give means loading you need to do unloading you need to do inside that the queries and all you need to make it depends upon your use case let's say if I give you web server analytical logs means web server analytical things means okay you need to go for something transformation most probably transformation we can't do here some analytical work only I'll give you so everything is based out of queries you need to run it depends upon your use case okay is it clear let me to show you the unloading progress okay syntax will be just listen redshift unload to s3 see unload select start from menu okay my bucket name is this my bucket name so inside my ticket unload data is not available there unload folder is not available there when you file also not available there let's verify and run this command okay so first let's get into our bucket and verify the details this three bucket ticket is there any folder is there called unload no so now go down execute that particular command in our corresponding unload go to your command prompt and load I think one word okay now refresh and just check time out select start from when you sorry select star from select star from okay so now go back to your syntax control c see what is it unload completed it it unloaded the 202 records ok go down verify refresh let check here unload one new folder has been created if you just check here all your files are available as a this is how you need to do your unload okay these are all the syntax I'm not sure how will I give you the syntax yeah so it is based out of spark just listen it is based out of spark okay so how it will run in the sense it will create a multiple files as a single file it won't save it will create it as a multiple files maybe one file uh two not to record right maybe hundred hundred on one something it will be there as a multiple files only it will save it because distributed data will be distributed in multiple machine while unloading also depends upon the data that will be available on multiple machine it need to take out and did it and it will deliver you as a multiple file anything okay so how will i forward this command just give anyone employee id 23 23 23 so good so whatever the command I used all the commands are available in this okay so just check okay anything any queries so what I'm suggesting you before getting into like project get an abort postgreSQL what are all the key things are available there in postgreSQL okay again it will come under a relational database only but just get to know and get into your project i think guru project only five projects i'll give you can share it and you can do that okay anything simul see step by step you need to do it in a proper manner or else you will get a error just get a clarity on what you are doing\n",
      "üìå Summarizing transcript with LLaMA...\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "The meeting transcript appears to be a discussion about loading and unloading data in Amazon Redshift, a data warehousing tool. Here's a summary of the key points:\n",
      "\n",
      "**Loading Data:**\n",
      "\n",
      "* The speaker explains that loading data into Redshift is similar to a normal data warehousing process.\n",
      "* The speaker provides a list of tasks to perform when loading data, including:\n",
      "\t+ Changing the route to all things Ctrl-C.\n",
      "\t+ Changing the region and bucket name.\n",
      "\t+ Verifying the syntax and format of the data.\n",
      "\t+ Loading the data into Redshift using the `COPY` command.\n",
      "* The speaker emphasizes the importance of understanding the integration part of Redshift, including linking it with S3, RDS, and EC2.\n",
      "\n",
      "**Unloading Data:**\n",
      "\n",
      "* The speaker explains that unloading data from Redshift is similar to a normal data warehousing process.\n",
      "* The speaker provides an example of unloading data using the `UNLOAD` command.\n",
      "* The speaker explains that the `UNLOAD` command will create multiple files as a single file, depending on the data size and distribution.\n",
      "* The speaker emphasizes the importance of understanding the syntax and format of the data when unloading it.\n",
      "\n",
      "**Best Practices:**\n",
      "\n",
      "* The speaker suggests that before starting a project, it's essential to get familiar with PostgreSQL and its key features.\n",
      "* The speaker recommends practicing loading and unloading data in a controlled environment before working on a real project.\n",
      "* The speaker emphasizes the importance of understanding the integration part of Redshift and its dependencies.\n",
      "\n",
      "Overall, the meeting transcript provides a brief overview of loading and unloading data in Amazon Redshift, as well as some best practices for working with the tool.\n",
      "\n",
      "üíæ Summary saved as meeting_summary.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: Test with a video\n",
    "    video_path = \"editedvideotrail.mp4\"  # <-- replace with your file path\n",
    "    transcript_text = transcribe_video(video_path)\n",
    "    print(\"\\n--- FULL TRANSCRIPT ---\\n\")\n",
    "    print(transcript_text)\n",
    "\n",
    "    summary = summarize_text(transcript_text)\n",
    "    print(\"\\n--- SUMMARY ---\\n\")\n",
    "    print(summary)\n",
    "\n",
    "    # Save summary\n",
    "    with open(\"meeting_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "    print(\"\\nüíæ Summary saved as meeting_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
